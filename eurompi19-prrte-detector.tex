%%%% Proceedings format for most of ACM conferences (with the exceptions listed below) and all ICPS volumes.
\documentclass[sigconf]{acmart}
\usepackage{algorithm}
\usepackage[export]{adjustbox}
\usepackage[noend]{algpseudocode}
\usepackage{todonotes}
\usepackage{xspace}
\usepackage{listings}

\newcommand{\mpifunc}[1]{\lstinline"MPI_#1"\xspace}
\newcommand{\prrte}[0]{\textsc{PRRTE}\xspace}
\newcommand{\pmix}[0]{\textsc{PMIx}\xspace}
\newcommand{\orte}[0]{\textsc{Open~RTE}\xspace}
\newcommand{\ompi}[0]{\textsc{Open~MPI}\xspace}
\newcommand{\ulfm}[0]{\textsc{ULFM}\xspace}
\newcommand{\mpi}[0]{\textsc{MPI}\xspace}
\newcommand{\oshmem}[0]{\textsc{OpenSHMEM}\xspace}

%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% Proceedings format for SIGPLAN conferences 
% \documentclass[sigplan, anonymous, review]{acmart}

%%%% Proceedings format for SIGCHI conferences
% \documentclass[sigchi, review]{acmart}

%%%% To use the SIGCHI extended abstract template, please visit
% https://www.overleaf.com/read/zzzfqvkmrfzn

%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2019}
\acmYear{2019}
\setcopyright{licensedusgovmixed}
\acmConference[EuroMPI '19]{EuroMPI '19: ACM International Conference Proceeding Series}{September 11th-13th, 2019 }{Zurich, Switzerland}
\acmPrice{15.00}
\acmDOI{10.1145/1122445.1122456}
\acmISBN{978-1-4503-9999-9/18/06}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}

%
% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}

%
% end of the preamble, start of the body of the document source.
\begin{document}

%
% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title{Runtime Level Failure Detection and Propagation in HPC Systems}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Dong Zhong}
%\orcid{1234-5678-9012}}
\email{dzhong@vols.utk.edu}
\orcid{0000-0002-7651-2059}
\affiliation{%
  \institution{The University of Tennessee}
  \streetaddress{1122 Volunteer Blvd}
  \city{Knoxville}
  \state{TN}
  \postcode{37996}
  \country{USA}
}
\author{Aurelien Bouteiller}
\orcid{0000-0001-5108-509X}
\email{bouteill@icl.utk.edu}
\affiliation{%
  \institution{The University of Tennessee}
  \streetaddress{1122 Volunteer Blvd}
  \city{Knoxville}
  \state{TN}
  \postcode{37996}
  \country{USA}
}
\author{George Bosilca}
\orcid{0000-0003-2411-8495}
\email{bosilca@icl.utk.edu}
\affiliation{%
  \institution{The University of Tennessee}
  \streetaddress{1122 Volunteer Blvd}
  \city{Knoxville}
  \state{TN}
  \postcode{37996}
  \country{USA}
}
\author{Xi Luo}
%\orcid{1234-5678-9012}}
\email{xluo12@vols.utk.edu}
\affiliation{%
  \institution{The University of Tennessee}
  \streetaddress{1122 Volunteer Blvd}
  \city{Knoxville}
  \state{TN}
  \postcode{37996}
  \country{USA}
}
%\author{Jack J. Dongarra}
%\orcid{0000-0003-3247-1782}
%\email{dongarra@icl.utk.edu}
%\affiliation{%
%  \institution{The University of Tennessee}
%  \streetaddress{1122 Volunteer Blvd}
%  \city{Knoxville}
%  \state{TN}
%  \postcode{37996}
%  \country{USA}
%}
%
% By default, the full list of authors will be used in the page headers. Often, this list is too long, and will overlap
% other information printed in the page headers. This command allows the author to define a more concise list
% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Zhong, Bouteiller, et al.}

%
% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}
\todo{ fix the expression and maybe the logic ?}
As the scale of High Performance Computing (HPC) system continues to grow, with more and more nodes deployed, mean-time-to-failure (MTTF) of those HPC systems is dramatic impacted (become lower and lower/drops). In order to efficiently run long time computing job on these systems, fault tolerance become a prime challenge/technology. In this paper, we present the design and implementation of an efficient runtime-level failure detection and propagation strategy targeting exascale systems. The detection is able to detect both node failure and process failure. A ring topology is maintained by allowing one node sending and receiving periodically heartbeat to/from another node to detect a node failure(observing another single node). For process failure each host node (is in charge of monitoring) monitors its children processes. The propagation use a reliable broadcast method over a binomial graph(an arbitrary communication topology) to distribute error message to applications, guarantees a logarithmic propagate time. We focus primarily on the most widely used programming paradigms \pmix Reference RunTime Environment(\prrte), the algorithms and strategies proposed have a larger scope of most distributed programming environment. Experiments on different machines successfully demonstrated the algorithm performs well.
\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010520.10010521.10010537</concept_id>
<concept_desc>Computer systems organization~Distributed architectures</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010520.10010521.10010542.10010546</concept_id>
<concept_desc>Computer systems organization~Heterogeneous (hybrid) systems</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010520.10010575.10010577</concept_id>
<concept_desc>Computer systems organization~Reliability</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010520.10010575.10011743</concept_id>
<concept_desc>Computer systems organization~Fault-tolerant network topologies</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10010940.10011003.10011005</concept_id>
<concept_desc>Software and its engineering~Software fault tolerance</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011041.10011048</concept_id>
<concept_desc>Software and its engineering~Runtime environments</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Distributed architectures}
\ccsdesc[300]{Computer systems organization~Heterogeneous (hybrid) systems}
\ccsdesc[300]{Computer systems organization~Reliability}
\ccsdesc[300]{Computer systems organization~Fault-tolerant network topologies}
\ccsdesc[300]{Software and its engineering~Software fault tolerance}
\ccsdesc[300]{Software and its engineering~Runtime environments}

%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{fault tolerance, failure detection, reliable broadcast, message propagation, HPC runtime system}

%
% This command processes the author and affiliation and title information and builds
% the first part of the formatted document.
\maketitle

\section{Introduction}\label{sec:intro}

The complexity and vastness of the questions posed by modern science has
fueled the emergence of an era where exploring the boundaries of mater, life,
and human knowledge require large instruments, either to perform the
experiments, collect the observation, and in the
case of High Performance Computing (HPC), perform the compute intensive
analysis of scientific data. As the march of science continues, small and
easy problem have already been solved, and significant advances increasingly
require tackling finer grain, more accurate problems, which entails that the
compute workloads are getting larger, fueling an unending need for larger
HPC systems.

In turn, facing hard limits on power consumption and chip frequency,
HPC architects have been forced to embrace massive parallelism as well as
a deeper and more complex component hierarchy (e.g., non-uniform memory architectures,
GPU accelerated nodes) to continue the growth in available compute capabilities.
This has stressed the traditional HPC software infrastructure in different 
ways, but it notably put to prominence two different issues that had been 
largely dormant in the last two decades: fault tolerance and novel programming 
models.

The Message Passing Interface (MPI) has been instrumental in 
permitting the efficient programming of massively parallel systems, scaling 
along from early systems with tens of processors to nowadays systems routinely 
encompassing hundreds of thousands of cores. As failures become more 
common on large and complex systems, The \mpi standard is in the process of 
evolving to integrate fault tolerance capabilities, as proposed in the 
User-Level Failure Mitigation (\ulfm) specification draft~\cite{Bland2013}, or 
various efforts to integrate tightly checkpoint-restart with \mpi~\cite{reinit18}.
The second stress comes from programming systems that are inherently
hierarchical. This has brought forth a renaissance in the field of
programming models, leading to a variety of contenders challenging the
hegemony of \mpi as the sole method of harnessing the power of parallel systems.
Naturally, these alternatives to \mpi also have to handle fault tolerance~\cite{7161563, doi:10.1177/1094342016669416, shmem-ft15, 10.1007/978-3-319-50995-2_5, X10-ft16}. 
In addition, the convergence
between Big-Data infrastructure and the HPC infrastructure, as well as
the emergence of machine learning as a massive consumer of compute
capabilities is gathering around HPC systems new communities that 
have long held expectations that the infrastructure provides resilience as a core feature~\cite{hadoop-ft}.

A feature that's commonly needed by these communities with a vested
interest in fault tolerance is the capability to efficiently, timely and
accurately detect and report failures, so that they can manifest as
error codes from the programming interface, or trigger implicit recovery
actions. In prior works~\cite{George16}, we have designed a tailor-made
failure detector for \mpi that deploys finely tuned optimizations to
improve its performance. These optimizations are unfortunately strongly
tied to the \mpi internal infrastructure. For example, a key parameter to 
the performance of that detector is the
access to low-level remote memory access routines, which may not be typically
available in a less \mpi-centric context. Similar concepts could be
applied to other HPC networking interfaces (e.g., \oshmem), but at 
the expense of a major infrastructure rewrite for each and every one of 
them. In this paper, we test the hypothesis that a fully-dedicated \mpi 
solution is not necessary to achieve great accuracy and performance, and 
that a generic failure detection solution, provided by an external runtime, 
entity that does not have access to the \mpi context and communication conduits
can deliver a similar level of service. In order to test that hypothesis, 
and further, to define how generic one can make the solution, we designed 
a failure detection component in the \pmix~\cite{CASTAIN18} runtime reference implementation
(\prrte), which is a fully fledged runtime that can deploy, monitor and 
serve multiple HPC networking stack clients like \ompi, or the 
reference \oshmem implementation. We then compare this generic 
failure detection service with the fully-dedicated \mpi detector from 
\ulfm \ompi on one hand, and with the Scalable Weakly-Consistent Infection-style Membership (SWIM) protocol on the other hand, the later standing
as a state-of-the-art detector for unstructured peer-to-peer systems. Henceforth 
we highlight that there is a performance tradeoff in generality, but 
satisfactory level of performance can be achieved in a portable and reusable 
component that can satisfy the needs of a variety of HPC networking systems. 

\todo{
Make sure all contributions are explicit: 
design a general purpose fault tolerance framework in \pmix 
explore the performance tradeoff incurred by generality
demonstrate that the runtime based failure detector can compete with the 
highly tuned/MPI specific \ulfm detector, and outperforms completely generic detectors.
}

The rest of this paper is organized as follows. In Section~\ref{sec:motivation}
we describe motivating use cases and give some background on the \mpi specific 
failure detector implementation in \ulfm. In Section~\ref{sec:design} we describe 
the algorithms and implementation details of our generic failure detector.
Section~\ref{sec:experiments} describes the performance and accuracy comparison
between three different failure detectors providing a distinct tradeoff 
on the general to specific scale. We then discuss related works\todo{make sure corresponds to final organization} in 
Section~\ref{sec:related} before we conclude.

\todo{decide if move to the end}
\section{Related Work}\label{sec:related}
In this section, we survey related work on large scale distributed runtime environments, different kinds of heartbeat based and random gossip based failure detectors, together with reliable broadcast algorithms to propagate fault information.

\subsection{Runtime Enviroments}
A wide range of approach to the problem of exascale distributed computing runtime environments have been studied, each primarily emphasize a particular key aspect of the overall problem. 

MPICH provides several runtime environments, such as MPD~\cite{Butler00}, Hydra~\cite{MPICH14} and Gforker~\cite{MPICH14}. MPD connects nodes through a ring topology but it is not resilient, two node failures could separate nodes into two separates groups that prevents communication with one another. Another drawback of MPD is that this approach has proved to be non-scalable~\cite{Bosilca11}.
Hydra scales well for large number of processes on a single node and interacts efficiently with hybrid programming models that combine \mpi and threads, but it does not behave well on multi-node environment.\todo{citation needed?} \orte~\cite{Castain05, Jeffrey12} is the \ompi runtime environment to launch, monitor, and kill parallel jobs, as well as managing I/O forwarding. It also connects daemons through various topologies, however the communication is not reliable. Also, all those runtimes have limited applicability outside of the related \mpi implementation 
that has motivated their creation. 

The \prrte runtime serves as the demonstrator and reference implementation
for the \pmix specification~\cite{CASTAIN18}. Technically it is a fork of
the \orte runtime, and thus inherit most of its capabilities to launch
and monitor \mpi jobs. Thanks to its well specified \pmix interface. \prrte
can deploy not only the related \mpi implementation of project from which it
is spun, but also a wide variety of parallel applications (MPI based or not).
Although \prrte provides rudimentary support for clients fault detection and reporting, 
detection of failed nodes is rudimentary and slow, and the reporting 
communication topology is not itself resilient. 
The current work expands on the existing capabilities of \prrte by adding
advanced failure detection and reporting methodologies that can operate 
despite the failure of the runtime daemon themselves.

\subsection{Failure Detection}
Research in the areas of failure detection has been extensively studied. Chandra and Toueg~\cite{Chandra96} proposed the first unreliable failure detector oracle that could solve consensus and atomic broadcast problem for reliable distributed systems. Many implementations~\cite{Wei02, Larrea00, Kawazoe97} based on this oracle work are using all-to-all heartbeat patterns where every node periodically communicates with all nodes. However, those implementations are not scalable for lager systems with hundreds of nodes. The gossip-style protocol \cite{van98, Ranganathan01, Gupta01, Abhinandan02}, in which nodes pick at random other nodes to monitor and exchange information, is a popular approach for failure detection in unstructured systems were the group membership 
is not a-priori established. However, these gossip methods do not work well with very large number of node crashes, and the time to detect a specific failure is not strictly bounded given the 
random nature of the protocol. Furthermore, the gossip methods have the disadvantage of 
generating a large number of redundant detection and gossip messages that decrease the scalability.

Recently, our previous work \cite{George16} propose an observation ring based detector for HPC systems. 
the simulation results demonstrate the efficient of the algorithm, however the implementation in \ulfm is an application level failure detection strategy, which can only detect \mpi process failures. The 
implementation employs multiple optimization and shortcuts that are possible because it is 
tightly and deeply integrated within the \mpi library. For example, limitations on the 
accuracy of the detector when the \mpi implementation is not actively communicating 
are circumvented by using Remote Memory Access primitives (RMA) which are
initially provided for supporting the \mpi communication; the operational 
mode and accuracy of the detector can be impacted by the thread model used 
during the \mpi initialization (e.g., \mpifunc{MPI_THREAD_SINGLE} may result in 
lower accuracy of the detector); in manycore systems, every \mpi process 
is observed and reported as an independent entity, which can impart that the 
overhead scales with the number of \mpi processes rather than the number of 
compute nodes. This resilient \prrte work 
exceed these limitations and has the capability to detect both process and 
node failures with a smaller observation topology, and is not limited to
MPI application only. 

\subsection{Reliable Broadcast}
Notable efforts have been made for fault-tolerant communication based on logical network typologies \cite{Luo18}. A fully connected topology can handle large number of failures but it cannot be scalable since it sends too many messages. At the other extreme, a ring topology is good for scalability
but offers a poor propagation latency and suffers in scenarios with multiple node failures.
A more recent effort is k-nomial circulant graph~\cite{Angskun07, Pava11} which is a balance between the previous two methods. 
Among k-nomial circulant graphs, the binomial graph (BMG) has the lowest diameter, which minimizes the number of hops for a dissemination to reach all 
processes. Also binomial graph scales well thanks to a regular, fixed degree graph (each node has the same number of neighbors). 
%
The broadcast routing approach in~\cite{Angskun07} uses a collection 
of binomial spanning trees with a fixed send ordering of neighbors. 
%The key-concept of fault-tolerant broadcasting is diameter of the topology, which is defined the longest shortest path between any two nodes in the graph. Binomial has the smallest diameter.
%Also binomial graph scales well with reasonable degree, regular graph and lowest diameter. 
%However when doing a reliable broadcast with message forwarding, 
%However when implementing a binomial graph, previous approach \cite{Angskun07} uses a fixed sending sequence to all the neighbours which has the disadvantage of not using high sending priority to unnotified nodes. Our work uses binomial graph with a reordering strategy when sending and forwarding messages, which means lower average reroute hops and message traffic density.

Our work uses a BMG with a reordering strategy when sending and forwarding messages that 
puts a high priority for messages that target leaves in previously effected 
binomial trees, which helps reaching not yet notified nodes quicker when intermediate 
hops have failed.\todo{that text needs another review/pass. But, not clear if want to talk about this in this paper.}
 
\section{Design and Implementation}\label{sec:design}

In this section, we describe the design of Resilient \prrte, our resilient mechanism has two components: a node level observation ring and a reliable broadcast overlay network. We also provide rationale for our design decisions, and implementation details.

\begin{figure}[h]
  \centering
  \includegraphics[trim=0.2cm 9.0cm 0.2cm 9cm,width=\linewidth]{PMIx_PRRTE.pdf}
  \caption{Resilient \prrte component architecture. The orange boxes represent the component we mainly use to add the resilient features. The dark blue colored boxes are new modules}\label{fig:prrte}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{server_client.pdf}
  \caption{Hierarchical notification of hosted processes through \pmix notification routines. The \prrte daemon represents the entire set of hosted processes in the detection and reliable broadcast topology and takes care of issuing local notifications.}\label{fig:hosted}
\end{figure}

\subsection{Model}
\texttt{\bf Failure model: } Node failure: the result of node crash, lost the daemon and all processes hosted on this node. Process failure: one or more serial or parallel processes due to a processor or node failure.

\texttt{\bf System model: } \prrte is a fork of the \ompi runtime, \orte~\cite{Castain05}. It is based on a Module Component Architecture (MCA) which permits easily extend or substitute the core subsystem with experimental features. As shown in in figure~\ref{fig:prrte}, within this architecture, each of the major subsystems is defined as an MCA framework, with a well-defined interface. Also each framework contains one or more components each representing a different implementation. Then the \orte work spun off to its own effort as a ``\verb|shim|`` between the application and the host environment that includes full support for the \pmix \cite{CASTAIN18} - an abstract set of interfaces by which not only applications and tools can interact with the resident system management stack (SMS), but also the various SMS components can interact with each other. 

\prrte provides an easy way of launching, running and monitoring \pmix-based applications outside of a \pmix-enabled environment on distributed systems. The \prrte overall architecture is out the scope of this paper, but we still want to mention some features embraced by it. First of all, \prrte's very first feature is Job controlling and Monitoring~\cite{Ralph15} which enables the application and SMS to coordinate the response to failure: termination of the job or a subset of processes, request replacement of nodes and processes, or continue execution at a lower setting. This provides the foundation of resilience strategy of our work. Another important feature is the \pmix Event Notification~\cite{Ralph002} : the resource manager or server can notify the application of events, the application processes can notify their server or SMS of issues. This provides the channel for locally propagation of error messages. Resilient \prrte mainly embraces two new features of failure detection and reliable broadcast for messages propagation globally and locally. We will give a detail introduction of the two features in the following sections.

\subsection{Detection of Process and Node Failures}

\begin{table}
  \caption{Parameters and notations}
  \label{tab:parameters}
  \begin{tabular}{ccl}
    \toprule
    Symbol & Description \\
    \midrule
    HNP & Head Node Process \\
    Daemon & Runtime environment process: the link  \\&  between the HNP and the application \\
    \texttt{\bf N} & Number of Daemon/nodes \\
    \texttt{Id} & The identifier of a Daemon \\
    \texttt{LIST\{ID\}} & List of known failed daemons' ID \\
    $\delta$ & Heartbeat period \\
    $\eta$ & Timeout for assuming a daemon failure\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}
\centering
\begin{minipage}{.23\textwidth}
  \centering
  \includegraphics[trim=3cm 8.0cm 3cm 8cm,width=\linewidth]{ring_detector.pdf}
  \captionof{figure}{Ring topology}
  \label{fig:Ring}
\end{minipage}%
\begin{minipage}{.23\textwidth}
  \centering
  \includegraphics[trim=3cm 8.0cm 3cm 8cm,width=\linewidth]{reconnet_cross.pdf}
  \captionof{figure}{Reconnect with failures}
  \label{fig:Reconnect Ring}
\end{minipage}
\end{figure}
We provide a strategy to detect both process failure and node failure in different ways. The \orte Daemon's Local Launch Subsystem (ODLS) is responsible for launching and monitoring local processes that are intended to launch the target applications' executable. Before the children processes execute the targets' executable, it need to set affinity of children processes according to a complex series of rules. This binding may fail in a myriad of different ways, the children processes will send a message to the parent indicating what happened (rendered error message) and the parent will read the message and analyze if this is a process failure and then use the runtime system's reporting mechanisms to display this error globally. 

For a general distributed system, nodes can communicate to any other nodes by sending message through communication channels. We arrange all N nodes to a logistic ring topology as in figure 3, which means, each particular node \textbf{\textit{p}} has a observer and a predecessor in the ring. The predecessor will send periodic heartbeat messages to \textbf{\textit{p}} with the period of $\delta$, at the same time \textbf{\textit{p}} will send heartbeat messages to its observer. For each node it emits heartbeats $m_1$, $m_2$, ... at time $\tau_1$, $\tau_2$, ... to its observer \textbf{\textit{q}}. Let $\tau_i' = \tau_i + t$. At any time \textbf{\textit{t}} $\in [\tau_i', \tau_{i+1}')$, \textbf{\textit{q}} believe \textbf{\textit{p}} is alive if it receive message $m_i$ or higher. Otherwise, \textbf{\textit{q}} suspect \textbf{\textit{p}} is failed, \textbf{\textit{q}} will prepare error information of \textbf{\textit{p}} and propagate this information. 

When the observer detects that the predecessor is failed, there are two major steps we do. First we need to reconnect the ring topology as in figure 4, the observer will search its own known failed list and figured out the first alive node \textbf{\textit{nq}} preceding in the ring, it set \textbf{\textit{nq}} as its predecessor and then send a request to \textbf{\textit{nq}} for being the new observer. The second step is failure propagation: the observer need to get access to the job's global mapping and binding information and find out all those processes hosted on predecessor node. After we get the list of all those affected processes and node, the observer calls the propagation component to delivery this message on node level to its neighbours, then observer will notify its local processes. Then, the observer updates its local known failure List\{\textit{ID}\}. For all those nodes who received the notification, each node need to forward this information, maintain its own List\{\textit{ID}\} and notify locally. As show in figure 1, we created a new module in error management as detector to enable this feature.

\subsection{Detection Broadcasting}
For broadcasting we use the scalable and fault tolerant topology binomial graph (BMG)~\cite{Angskun07}. BMG has good fault-tolerant properties such as optimal connectivity, low fault-diameter, strongly resilient and good optimal probability in failure cases. In order to continuing the job execution rather than aborting, we need let all participated nodes and applications be aware of failures, the three major mechanisms are 
\begin{enumerate}
  \item Observer fetch error information of failed node. Start the propagation globally by calling group communication component for broadcasting.
  \item Observer node broadcasts the information to all its local children. 
  \item All nodes forwarding the message upon receiving to make the broadcast reliable. Make sure all alive nodes receive the information at least once. 
\end{enumerate}

\begin{algorithm}implementation
\caption{Reliable broadcast algorithm }
\textbf{\textit{N}} \Comment{Number of alive nodes}\newline
\textbf{List\{$EID$\}} \Comment{Local list of known error processes' identifier}\newline
\textbf{\textit{msg}} \Comment{Message packed with jobid, process identifiers and state}\newline
\textbf{\textit{flag}} \Comment{Boolean flag indicating forward the message or not}\newline
\textbf{List\{$AID$\}} \Comment{Processes identifiers' hosted on failed daemon identifier }\newline

\begin{algorithmic}[1]
\Procedure{Broadcast}{ $i, N, msg$ }\Comment{Daemon \textbf{i} send error message to all its neighbours}
\For{ $k \gets 0$ to $\log_2 N$ }\Comment {Sending order not fixed}            
     \State {\textbf{i} send \textbf{msg} to  ( ($N$ + i + $2^k$) \textbf{mod} {N} ) }
     \State {\textbf{i} send \textbf{msg} to  ( ($N$ + i - $2^k$) \textbf{mod} {N} ) }
\EndFor
\EndProcedure
\end{algorithmic}

\begin{algorithmic}[1]
\Procedure{Forwarding}{ $flag, msg$ }\Comment{Daemon \textbf{j} receives msg, forwarding and notify locally }
\If {$flag == true$}
    \State Broadcast( $j, N, msg$ )
    \State Update \textbf{List\{$EID$\}}
    \State Notify locally
\EndIf
\EndProcedure
\end{algorithmic}

\begin{algorithmic}[1]
\Procedure{Start propagation}{ $job, Eid, state$ }\Comment{Daemon \textbf{j} start propagation }
\If {( $Eid$ $\notin$ List\{$EID$\} )}
    \State Add $Eid$ to $msg$
    \State Get \textit{AID} and adde to $msg$
    \State Broadcast( $j, N, msg$ )
    \State Add $Eid$ \textit{AID} to List\{$EID$\}
\EndIf
\EndProcedure
\end{algorithmic}

\end{algorithm}

Group communication use out-of-band non-blocking peer to peer communicate channel as the basic message exchange mechanism, we assume the communication latency from any two directly connected nodes with same message size are equivalent. For node level broadcast, each node sends message to all its neighbours  as in BMG topology, message from any node we follow the sequence as in binomial spanning tree by which means that any node in BMG can always be delivered within $O(log_2^N)$ steps.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{BMG_seq.pdf}
  \caption{Binomial graph with 12 nodes}
\end{figure}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{reorder_span.pdf}
  \caption{Broadcast using binomial spanning tree from node 0, extra messages to neighbours are colored in blue }
\end{figure}

\begin{figure*}[h]
\centering
\begin{minipage}{.37\textwidth}
  \centering
  \includegraphics[width=\linewidth]{multi_pingping_pingpong_overhead.pdf}
  \label{fig:Ring}
\end{minipage}%
\begin{minipage}{.63\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Bcast_overhead_with_ulfm_max_col.pdf}
  \label{fig:Reconnect Ring}
\end{minipage}
\captionof{figure}{\prrte with fault tolerance overhead over \prrte and \ulfm using IMB}
\end{figure*}

Figure 5 shows the BMG with 12 nodes, node 0 starts the propagation and sends messages to all its six neighbours with the sequence marked. And Figure 6 shows the a spanning tree for broadcast originated from node 0 also with extra messages colored blue for resilient purpose. Those extra messages balanced the trade-offs between high-radix trees that allow more parallelism at each level in the tree and more network connections, and lower-radix deeper communication trees with reduced network congestion. The advantages of this new broadcast algorithm are:
\begin{enumerate}
  \item With excellent network connection brings higher parallelism: message to node \{10, 11, 7\} could arrive from any forwarding nodes at earlier time stamp than routed from spanning tree, this decrease the height of the tree, the average latency and the maximum of notification latency are smaller.
  \item With limited network connection: maintains the upper bound of broadcast to $O(log_2^{N})$. 
\end{enumerate}
For inter-node notification, children processes subscribe to a event handler with specific error code and will be triggered by the notification function when a daemon doing the locally notification. Also,applications can register to multiple events which enable the capability to receive notification from different jobs. This attribute is introduced in group communication and propagation component.

\section{Experiment Evaluation}\label{sec:experiments}

\subsection{Experiment Setup}
The experiment are conducted on two different machines: (1) \todo{more detail about nacl}a local cluster named NaCl, an Intel Xeon X5660 machine with 66 nodes, 12 cores per node. (2) NERSC's Cori
%~\cite{Cori01} 
-- a Cray XC40 with Intel Xeon "Haswell" processors and the Cray "Aries" high speed inter-node network, 32 cores per node. The resilient \prrte is based on \prrte(\#71ef547), with external \pmix(\#21d7c9). The comparison \ulfm is based on(\#77f9157). The experiments is repeated 30 times and we present the average. We use Intel MPI Benchmark\cite{IMB} for MPI performance measurements for point-to-point and global communications and deploy with one \mpi rank per core. For all experiments we use map-by node, bind-to core binding policy -- put sequential MPI processes on adjacent processor cores. The only exception is the P2P benchmark experiment we use map-by node, bind-by node -- put sequential MPI processes on adjacent nodes. \todo{add binding policy}

\subsection{Accuracy and Noise}
For the first experiment we want to implore the noise overhead generated by the heartbeat ring detector with different heartbeat period from milliseconds to seconds. We also investigate the accuracy of the detector with different heartbeat period. 

The accuracy experiment is conducted as: (1)\ Start with a high value of $ \eta $, and decrease $ \eta $ and corresponding $ \delta $ until we notice false positive. The default setting of $ \eta $ and $ \delta $ are $ \eta = \delta * 2 $. (2) Use a reasonable fixed value of  $ \delta $ , decrease  $ \eta $ until any node misses the deadline of a heartbeat sending.  \todo{pull the description of the methodology in to make it self contained} If the test is successful (no failure is detected when there is no injection, and all injected failure are correctly detected), then we decrease the heartbeat period and repeat the test until a false positive is reported. We explore detector accuracy with two experiments show false positive behavior on 64 nodes on NaCl: 
\begin{enumerate}
  \item No failure is injected but detector shows a node is gone, which means the observer doesn't receive any heartbeat since last time and timeout is reached. We explore the possible reasons for this result with two tests: (1) Test with no communication but compute-only application, all test succeeds until heartbeat period under 20 msec. (2) Test with IMB benchmark with heavy point-to-point communication and collectives, all tests succeed until the heartbeat period lower than 20 msec as above. We assume that the timeout is neither delayed by communication congestion nor compute pressure. We investigated the problem with smaller job size running on less nodes , it shows a better performance. We find out that daemons need some time to launch the processes when starting the job which caused the false positive. 
  \item No failure is injected and node missed its heartbeat sending deadline. With a reasonable timeout value, all daemons can send heartbeats successfully until $ \delta $ as low as 0.1 msec. 
\end{enumerate}

Based on the results we conclude that the granularity of heartbeat timeout is 40 millisecond limited by the latency for daemons to launch all the application processes. And the granularity of sending heartbeat is 0.1 millisecond.  

Figure 7 illustrates the overhead incurred with P2P and collective communications, test results come from Intel MPI benchmark(v2019.2). All IMB-MPI1 suits can be successfully conducted with no false detection with $ \delta \geq 1 ms $. For the P2P setting, we run the benchmark on two nodes will all cores included using the benchmark's "multi" option, which pairs cores from each node as groups to ensure that no matter how daemon are binded to cores the heartbeat emissions affect the performance of P2P communication. For the collective we run the benchmark on 64 nodes using all cores. Any single test we use message size from several bytes to mega bytes, for each message size the test lasts more than 20 seconds to ensure enough heartbeat emissions are occurred during the experiment. The \prrte overhead is calculated by using the result of benchmark suits (LAT): 
\begin{equation}
\frac{(\ {FT\_PRRTE_LAT - PRRTE\_LAT}\ )}{PRRTE\_LAT} 
\end{equation}
 From the figure we can see that the latency of performance and bandwidth performance are barely affected by heartbeats periods from milliseconds to seconds. Notably, when heartbeat emission frequent is less than 100 times per second, it has trivial influence on the system. When the heartbeat period value sets to 1 millisecond, the noise overhead for both P2P are less than three percentage. As for collective communication(Bcast Reduce and Allreduce) the noise overhead are less than eight percentage under the circumstance that the standard deviation of the benchmark itself is four percentage. Also we calculate the overhead of \ulfm without fault tolerance compared to \prrte, shows that \ulfm has about two percent overhead for P2P and collective communication respectively for all both small and big message sizes.

\subsection{Comparison with SWIM}
This section compares failure detection latency and scalability of our failure detector with SWIM~\cite{Abhinandan02}, using a random-probing based failure detection protocol and disseminates membership updates via network multi-cast. SWIM uses subgroups to probe to decrease the randomness and increases the scalability. To avoid false detection, SWIM uses suspicion mechanism, when a node does not reply direct or indirect probing in time, the initiator then judges this node as suspicious instead of failure, then broadcasts
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{HB_prrte_swim.pdf}
  \caption{Detection and Propagation delay compared to SWIM using Go-Memberlist}
\end{figure}
this suspicious information within the subgroup, if any node in the subgroup receives an acknowledge before the timeout, it will unmask the suspected node as alive, otherwise mark as failure. In order to improve the efficiency of multi-cast, SWIM uses a infection-style dissemination mechanism as information spreads in a manner analogous to the spread of gossip in society, or epidemic in the general population.

Figure 8 compares detection and propagation latency between our \prrte detector and SWIM detector of node failure. For the SWIM implementation, we use Go-Memberlist (\#a8f83c6) which is integrated with Go instead of mpi, however we use go-mpi binding to enable SWIM run as mpi application, we use mpi barrier to synchronize before injecting failures in SWIM, and SWIM reports failure through Go-Memberlist callback functions. We run all Memberlist tests only up to 256 processes which is the upper bound of SWIM member limited by maximum connection requests on Transmission Control Protocol (TCP) socket, however this implementation suffers from connection storm and cause consequently crash of continuous running. 

For \prrte, we also synchronize with barrier, and use our benchmark to inject node failure, detection and notification latency are collected on each process. We run all tests up to 768 processes on 64 nodes.We compare the latency of detection and propagation with different heartbeat period value, for all test we set the timeout equal to two times of heartbeat. We can clearly see that for \prrte the detection latency is around the middle of heartbeat and timeout between ($\delta$, $2*\delta$), and the receive of notification happens immediately after the detection which demonstrate the efficiency of our propagation algorithm. However for SWIM even with smaller number of processes the delay is still more than $10*\delta$. The variation of \prrte result comes from the randomness of when the node failure happened.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Scale_prrte_swim.pdf}
  \caption{Scalability compared to SWIM using Go-Memberlist}
\end{figure}

Figure 9 compare the scalability of the two detectors regard to the number of deployed processes. We set the heartbeat period to 0.5 second with timeout of 1 second. For smaller number of processes \prrte failure detector is stabilizing in approximately 0.75 second,a statistic behavior of the randomness of when the failure happened between two contiguous heartbeats, while the stabilization delay of SWIM is more than 10 times of heartbeat period. As the number of processes increases, \prrte's latency remains almost the same. But SWIM shows a linear increase which will be the bottleneck of scaling up(with the assumption it can solve the maximum connection requests limit). For \prrte with 4K processes the stabilization is still beyond the range of heart period and timeout.

\subsection{Comparison with \ulfm for Process Failures}
This section compares our failure detector with our previous work \ulfm~\cite{George16}, the \ulfm implementation also has two main components: process level detection ring, and propagation overlay with all launched processes. The detection ring is built at Byte Transfer Layer (BTL) level - which provides the portable low-level transport abstraction in \ompi. The current detection implementation provides several mechanisms to ensure the timely activation and delivery of heartbeats:
\begin{enumerate}
  \item Using a separate, library internal thread to send the heartbeats in order to be separated from the application's communication, this also avoid the possible heartbeat delay of compute intensive application. For receiver it need to poll BTL engine to check the aliveness of its successor. 
  \item Using RDMA put to raise a flag in the receiver's registered memory, by using the hardware accelerated put operations solves the problem of active polling BTL engine. 
  \item Using in-band detection for process to report unreachable error directly to the propagation component.
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Process_Failure_log_fit.pdf}
  \caption{Process failure detection and propagation delay compared to \ulfm}
\end{figure}

The propagation overlay is also built at BTL level, the small message size of propagation information contains a callback function index which ensures that the received process states update information can be analyzed by upon reception. This method provides independence from the \mpi semantic (including matching), however the overlay is constructed with all processes, which means that all participated process need to propagate/forward the error information, the lower bound of reaching any process is bounded by $\log_2({Number\ of\ Processes})$.  

However, \prrte process failure detection is implemented as the daemons are monitoring the lifeline of all local processes, this mechanism doesn't bring any pressure to the applications' communication resources, also doesn't need RDMA hardware accelerate support. About the propagation strategy of \prrte, the broadcast overlay is built at daemon level which highly decreases the number of participates, with less participates the total messages transferred and forwarded is much less than the case of \ulfm, also the lower bound is $\log_2({Number\ of\ Nodes})$. With the development of more powerful multi-core nodes, the benefit of node level propagation will be much more significant.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Cori_Process_Failure_fit.pdf}
  \caption{Process failure detection and propagation delay on Cori}
\end{figure}

Figure 10 compares the latency of process failure detection and propagation of \ulfm and \prrte. The sensitivity of \ulfm heartbeat without false detection is 10 milliseconds, but we want to compare the best performance from \ulfm using high speed in-band detection which is faster than using the lowest heartbeat period. For \prrte, the daemons are in charge of detecting any process failure. \prrte uses TCP to broadcast among daemons, each daemon uses the \pmix notification method to distribute the error information to all hosted processes. Experiments are conducted on our local cluster NaCl from 2 nodes to 64 nodes using all cores on each node, all processes are equalled mapped to node and bind to core, by doing this \ulfm can use the in-band shared communication for detection and propagation. We can see that our implementation gains almost the same performance as \ulfm but highly reduced the complexity, the detection and propagation time is less than 5 millisecond with different number of nodes start from 2 to 64 using TCP. For \ulfm the detection and propagation delay increase from 2 millisecond to 3 millisecond as the processes number increases with number of nodes using infiniband. For both \prrte and \ulfm the latency increase trend fit to $ a*\log_2(N) + b $, which can be easily scale up to hundreds of thousands of nodes. Similar result strengthens in figure 11 from Cori with more processes on each node, we can see that with 4K processes the detection and propagation latency is about 10 milliseconds. This result indicates the efficiency of our broadcast and propagation algorithm.  

\subsection{Node Failures Detection}
Future more, we want to extend the performance of our heartbeat detector  for node failure detection and propagation from  three perspectives. 
\begin{enumerate}
  \item Latency of detection time and propagation time with different heartbeat periods.
  \item The detection and propagation efficiency using fixed heartbeat and timeout period but with different number of nodes.
  \item Detection latency of multiple node failures.
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{PRRTE_ULFM_comparision.pdf}
  \caption{Daemon Failure detection and propagation delay compared to \ulfm with different heartbeat period}
\end{figure}

Figure 12 presents the behavior observed when injecting single daemon failure under different heartbeat period setting. We conducted the experiments on 64 nodes with 764 processes. For \prrte after synchronizing, we inject a node crash by choosing a process to kill its host using the parent process identifier. However, \ulfm doesn't have the capability of detecting node failure, we simulate "node crash" by kill the last process on that node. With the process ring detector this particular process failure will be detected by its observer on another node, this behavior is the same as \prrte node to detect node failure. Also we assume that the time different between the daemon failure and process failure is trivial which is confirmed later in the experiment. For the heartbeat period setting we starts at 30 millisecond to 0.5s for both \prrte and \ulfm. For all heartbeat periods we set $ \eta = \delta * 2 $. From the figure we can see that the latency for all cases are bound by $ (\eta - \delta,\eta) $.

Figure 13 shows single node failure detection and propagation performance with fixed heartbeat period $ \delta = 0.5s $ tested with different number of nodes. With a node failure, all processes hosted on this node will be affected, the observer node fetches and packs the information of all affected processes information then distribute the packed message.
\textbf{
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Daemon_prrte_only.pdf}
  \caption{Daemon Failure Detection and Propagation delay with different number of nodes}
\end{figure}}
However, for \ulfm all affected processes need be detected by their observers independently, this will suffer from the collisions on the reliable broadcast propagation. For the worst case, all affected processes are adjacent in the detection ring, for each affected process the ring need to re-connect and update, causes a linear increase of $ \eta $ to the detection latency with every process failure. From the figure see that \prrte can detect and propagate a node failure between (0.5s, 1s) for all tested number of node. 

The last experiment (Figure 14) investigates the effect of multiple node failures happened in the system. The test setting is the same as single node failure case, except we inject multiple node failures from children processes. For the first scenario, injecting failure to nonadjacent nodes, the detection and propagation are independently conducted by different observer nodes which is a parallel execution. And the results in the figure show that the latency is a constant value which is not affected by the number node failures. The worst scenario is to inject failures to contiguous N nodes, all failures are detected by a single observer (switches to observing the predecessor of the detected failed node). This shows a serial execution. For each node failure it will cause an extra delay of $ \eta $ to the detection latency of the following node failures. From the figure we can see the result matches a linear increase. This extrapolate that from fault tolerance perspective the system will be more stable and reliable by building the detection ring in a arbitrary layout of nodes, avoid choosing predecessor and successor from the same cabinet.  

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{multi_daemon_failures.pdf}
  \caption{Multiple process failures at the same time}
\end{figure}

\section{Application}\label{sec:motivation}
In this section, we present an actual use case of the resilient \prrte to support the functionality in \ulfm of revocation and correction of a broken communicator after node failure. For \ulfm the global communicator could be reconfigured after process failure detection, the failed processes are excluded from the global communicator using the \mpifunc{Comm_shrink} and are re-spawned using the \mpi function \mpifunc{Comm_spawn}. However, the job will be aborted with node failure, by using \prrte instead of \ompi runtime the job doesn't need to be terminated after node failure, also with the runtime level propagation mechanism the error message can be delivered to the job scope ( in this case the \mpi universe). With the notification \mpi application can get the consensus knowledge of failures, then application can use the recovery functions (Shrink, Respawn) provided by \ulfm to build new communicator without the processes on the failed node but with those newly spawned processes. 
\todo{add SHMEM, REINIT; thinking about moving this part as 2. Motivation}

\oshmem fault tolerance model is based on check-point and restart, that suits to the one-sided nature of PGAS programming model while leveraging features very specific to \oshmem. Resilient \prrte failure detection and propagation attributes provide the flexibility to application developers to modulate the frequency and placement within the application where the checkpoint may be introduced. Also the notification provides the trigger for recovery. 

EREINIT a global-restart failure recovery model by allowing a fast re-initialization of \mpi. This work is a co-design between MVAPICH and Slurm resource manager to add process and node failures detection and propagation features. The detection is using Slurm's health check mechanism by sending ping messages to nodes, propagation is implemented in a manner of forcing the controller to individually send the notification to children of the failed nodes. However this work is highly depends on one particular resource manager Slurm has the limitation to run only on Slurm machine, also it use a inefficient propagation method. Our resilient \prrte work can exceed the bottleneck of EREINIT(Slurm machine only) to run on machines with different resource managers(Slurm, PBS, LSF, TORQUE, etc). Also with our efficient propagation algorithm the notification will be much faster advances the stabilization and recovery time of EREINIT.

\section{Conclusion}\label{sec:conclusion}
Failure detection and propagation is a critical service for resilient systems. In this work, we present an efficient failure detection and propagation design and implementation for distributed systems integrated within \pmix\_\prrte. The process and node failure detection strategy presented in this work depends on heartbeats and timeouts, and communication bound to provide a reliable solution that works at scale without constraints of the kind of faults. Our design and implementation takes into account of the complicate relationship and trade-off between system overhead, detection efficient and risks: low detection time requires frequent emission of heartbeats messages, increasing the system noise and the risk of false positive. Our solution addresses those concerns and capable of tolerating high frequency node and process failures with the help of high performance reliable broadcast notification can quickly disseminate the fault information, our results from different machines and benchmarks compared to two other related work shows that Resilient \prrte significantly advances the state of art with respect to efficient detection and propagation of failures. This runtime level failure tolerate implementation breaks the limit of running \mpi  application only and support all \pmix-based application which is more extensive than\mpi scope.   


%\section{Acknowledgments}
%
% The acknowledgments section is defined using the "acks" environment (and NOT an unnumbered section). This ensures
% the proper identification of the section in the article metadata, and the consistent spelling of the heading.
\begin{acks}
%CAARES and ECP OMPIX
This material is based upon work supported by the National Science Foundation under Grant No. (1725692); and the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the 
U.S. Department of Energy Office of Science and the National Nuclear Security Administration.    
\end{acks}

%
% The next two lines define the bibliography style to be used, and the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}
% 
% If your work has an appendix, this is the place to put it.
\appendix
\end{document}
