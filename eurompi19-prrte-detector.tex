%%%% Proceedings format for most of ACM conferences (with the exceptions listed below) and all ICPS volumes.
\documentclass[sigconf]{acmart}
\usepackage{algorithm}
\usepackage[export]{adjustbox}
\usepackage[noend]{algpseudocode}
\usepackage{todonotes}
\usepackage{xspace}
\usepackage{listings}
\usepackage{subcaption}

\newcommand{\mpifunc}[1]{\lstinline"MPI_#1"\xspace}
\newcommand{\prrte}[0]{\textsc{PRRTE}\xspace}
\newcommand{\pmix}[0]{\textsc{PMIx}\xspace}
\newcommand{\orte}[0]{\textsc{Open~RTE}\xspace}
\newcommand{\ompi}[0]{\textsc{Open~MPI}\xspace}
\newcommand{\ulfm}[0]{\textsc{ULFM}\xspace}
\newcommand{\mpi}[0]{\textsc{MPI}\xspace}
\newcommand{\oshmem}[0]{\textsc{OpenSHMEM}\xspace}
\newcommand{\ourwork}[0]{\textsc{Daemon}$^\#$\xspace}

\newcommand{\imb}[0]{\textsc{IMB}\xspace}

%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% Proceedings format for SIGPLAN conferences 
% \documentclass[sigplan, anonymous, review]{acmart}

%%%% Proceedings format for SIGCHI conferences
% \documentclass[sigchi, review]{acmart}

%%%% To use the SIGCHI extended abstract template, please visit
% https://www.overleaf.com/read/zzzfqvkmrfzn

%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2019}
\acmYear{2019}
\setcopyright{licensedusgovmixed}
\acmConference[EuroMPI '19]{EuroMPI '19: ACM International Conference Proceeding Series}{September 11th-13th, 2019 }{Zurich, Switzerland}
\acmPrice{15.00}
\acmDOI{10.1145/1122445.1122456}
\acmISBN{978-1-4503-9999-9/18/06}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}

%
% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}

%
% end of the preamble, start of the body of the document source.
\begin{document}

%
% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title{Runtime Level Failure Detection and Propagation in HPC Systems}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Dong Zhong}
%\orcid{1234-5678-9012}}
\email{dzhong@vols.utk.edu}
\orcid{0000-0002-7651-2059}
\affiliation{%
  \institution{The University of Tennessee}
  \streetaddress{1122 Volunteer Blvd}
  \city{Knoxville}
  \state{TN}
  \postcode{37996}
  \country{USA}
}
\author{Aurelien Bouteiller}
\orcid{0000-0001-5108-509X}
\email{bouteill@icl.utk.edu}
\affiliation{%
  \institution{The University of Tennessee}
  \streetaddress{1122 Volunteer Blvd}
  \city{Knoxville}
  \state{TN}
  \postcode{37996}
  \country{USA}
}
\author{George Bosilca}
\orcid{0000-0003-2411-8495}
\email{bosilca@icl.utk.edu}
\affiliation{%
  \institution{The University of Tennessee}
  \streetaddress{1122 Volunteer Blvd}
  \city{Knoxville}
  \state{TN}
  \postcode{37996}
  \country{USA}
}
\author{Xi Luo}
%\orcid{1234-5678-9012}}
\email{xluo12@vols.utk.edu}
\affiliation{%
  \institution{The University of Tennessee}
  \streetaddress{1122 Volunteer Blvd}
  \city{Knoxville}
  \state{TN}
  \postcode{37996}
  \country{USA}
}
%\author{Jack J. Dongarra}
%\orcid{0000-0003-3247-1782}
%\email{dongarra@icl.utk.edu}
%\affiliation{%
%  \institution{The University of Tennessee}
%  \streetaddress{1122 Volunteer Blvd}
%  \city{Knoxville}
%  \state{TN}
%  \postcode{37996}
%  \country{USA}
%}
%
% By default, the full list of authors will be used in the page headers. Often, this list is too long, and will overlap
% other information printed in the page headers. This command allows the author to define a more concise list
% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Zhong, Bouteiller, et al.}

%
% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}
\todo{ fix the expression and maybe the logic ?}
As the scale of High Performance Computing (HPC) system continues to grow, with more and more nodes deployed, mean-time-to-failure (MTTF) of those HPC systems is dramatic impacted (become lower and lower/drops). In order to efficiently run long time computing job on these systems, fault tolerance become a prime challenge/technology. In this paper, we present the design and implementation of an efficient runtime-level failure detection and propagation strategy targeting exascale systems. The detection is able to detect both node failure and process failure. A ring topology is maintained by allowing one node sending and receiving periodically heartbeat to/from another node to detect a node failure(observing another single node). For process failure each host node (is in charge of monitoring) monitors its children processes. The propagation use a reliable broadcast method over a binomial graph(an arbitrary communication topology) to distribute error message to applications, guarantees a logarithmic propagate time. We focus primarily on the most widely used programming paradigms \pmix Reference RunTime Environment(\prrte), the algorithms and strategies proposed have a larger scope of most distributed programming environment. Experiments on different machines successfully demonstrated the algorithm performs well.
\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010520.10010521.10010537</concept_id>
<concept_desc>Computer systems organization~Distributed architectures</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010520.10010521.10010542.10010546</concept_id>
<concept_desc>Computer systems organization~Heterogeneous (hybrid) systems</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010520.10010575.10010577</concept_id>
<concept_desc>Computer systems organization~Reliability</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010520.10010575.10011743</concept_id>
<concept_desc>Computer systems organization~Fault-tolerant network topologies</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10010940.10011003.10011005</concept_id>
<concept_desc>Software and its engineering~Software fault tolerance</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011041.10011048</concept_id>
<concept_desc>Software and its engineering~Runtime environments</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Distributed architectures}
\ccsdesc[300]{Computer systems organization~Heterogeneous (hybrid) systems}
\ccsdesc[300]{Computer systems organization~Reliability}
\ccsdesc[300]{Computer systems organization~Fault-tolerant network topologies}
\ccsdesc[300]{Software and its engineering~Software fault tolerance}
\ccsdesc[300]{Software and its engineering~Runtime environments}

%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{fault tolerance, failure detection, reliable broadcast, message propagation, HPC runtime system}

%
% This command processes the author and affiliation and title information and builds
% the first part of the formatted document.
\maketitle

\section{Introduction}\label{sec:intro}

The complexity and vastness of the questions posed by modern science has
fueled the emergence of an era where exploring the boundaries of mater, life,
and human knowledge require large instruments, either to perform the
experiments, collect the observation, and in the
case of High Performance Computing (HPC), perform the compute intensive
analysis of scientific data. As the march of science continues, small and
easy problem have already been solved, and significant advances increasingly
require tackling finer grain, more accurate problems, which entails that the
compute workloads are getting larger, fueling an unending need for larger
HPC systems.

In turn, facing hard limits on power consumption and chip frequency,
HPC architects have been forced to embrace massive parallelism as well as
a deeper and more complex component hierarchy (e.g., non-uniform memory architectures,
GPU accelerated nodes) to continue the growth in available compute capabilities.
This has stressed the traditional HPC software infrastructure in different 
ways, but it notably put to prominence two different issues that had been 
largely dormant in the last two decades: fault tolerance and novel programming 
models.

The Message Passing Interface (MPI) has been instrumental in 
permitting the efficient programming of massively parallel systems, scaling 
along from early systems with tens of processors to nowadays systems routinely 
encompassing hundreds of thousands of cores. As failures become more 
common on large and complex systems, The \mpi standard is in the process of 
evolving to integrate fault tolerance capabilities, as proposed in the 
User-Level Failure Mitigation (\ulfm) specification draft~\cite{Bland2013}, or 
various efforts to integrate tightly checkpoint-restart with \mpi~\cite{reinit18}.
The second stress comes from programming systems that are inherently
hierarchical. This has brought forth a renaissance in the field of
programming models, leading to a variety of contenders challenging the
hegemony of \mpi as the sole method of harnessing the power of parallel systems.
Naturally, these alternatives to \mpi also have to handle fault tolerance~\cite{7161563, doi:10.1177/1094342016669416, shmem-ft15, 10.1007/978-3-319-50995-2_5, X10-ft16}. 
In addition, the convergence
between Big-Data infrastructure and the HPC infrastructure, as well as
the emergence of machine learning as a massive consumer of compute
capabilities is gathering around HPC systems new communities that 
have long held expectations that the infrastructure provides resilience as a core feature~\cite{hadoop-ft}.

A feature that's commonly needed by these communities with a vested
interest in fault tolerance is the capability to efficiently, timely and
accurately detect and report failures, so that they can manifest as
error codes from the programming interface, or trigger implicit recovery
actions. In prior works~\cite{George16}, we have designed a tailor-made
failure detector for \mpi that deploys finely tuned optimizations to
improve its performance. These optimizations are unfortunately strongly
tied to the \mpi internal infrastructure. For example, a key parameter to 
the performance of that detector is the
access to low-level remote memory access routines, which may not be typically
available in a less \mpi-centric context. Similar concepts could be
applied to other HPC networking interfaces (e.g., \oshmem), but at 
the expense of a major infrastructure rewrite for each and every one of 
them. In this paper, we test the hypothesis that a fully-dedicated \mpi 
solution is not necessary to achieve great accuracy and performance, and 
that a generic failure detection solution, provided by an external runtime, 
entity that does not have access to the \mpi context and communication conduits
can deliver a similar level of service. In order to test that hypothesis, 
and further, to define how generic one can make the solution, we designed 
a multi-level failure detection algorithm, called \ourwork, that operates
within the runtime infrastructure to monitor both node and process 
failures. We implemented that algorithm as a component in the \pmix~\cite{CASTAIN18} runtime reference implementation (\prrte), which is a fully fledged runtime that is 
used in production to deploy, monitor and 
serve multiple HPC networking stack clients like \ompi, or the 
reference \oshmem implementation. We then compare this generic 
failure detection service with the fully-dedicated \mpi detector from 
\ulfm \ompi on one hand, and with the Scalable Weakly-Consistent Infection-style Membership (SWIM) protocol on the other hand, the later standing
as a state-of-the-art detector for unstructured peer-to-peer systems. Henceforth 
we highlight that there is a performance tradeoff in generality, but 
satisfactory level of performance can be achieved in a portable and reusable 
component that can satisfy the needs of a variety of HPC networking systems. 

The rest of this paper is organized as follows. In Section~\ref{sec:motivation}
we describe motivating use cases and give some background on the \mpi specific 
failure detector implementation in \ulfm. In Section~\ref{sec:design} we describe 
the algorithms and implementation details of our generic failure detector.
Section~\ref{sec:experiments} describes the performance and accuracy comparison
between three different failure detectors providing a distinct tradeoff 
on the general to specific scale. We then discuss related works \todo{make sure corresponds to final organization} in 
Section~\ref{sec:related} before we conclude.

\todo{decide if move to the end}
\section{Related Work}\label{sec:related}
In this section, we survey related work on large scale distributed runtime environments, different kinds of heartbeat based and random gossip based failure detectors, together with reliable broadcast algorithms to propagate fault information.

\subsection{Runtime Environments}
A wide range of approaches to the problem of exascale distributed computing runtime environments have been studied, each primarily emphasize a particular key aspect of the overall problem. 

MPICH provides several runtime environments, such as MPD~\cite{Butler00}, Hydra~\cite{MPICH14} and Gforker~\cite{MPICH14}. MPD connects nodes through a ring topology but it is not resilient, two node failures could separate nodes into two separates groups that prevents communication with one another. Another drawback of MPD is that this approach has proved to be non-scalable~\cite{Bosilca11}.
Hydra scales well for large number of processes on a single node and interacts efficiently with hybrid programming models that combine \mpi and threads. While Hydra can monitor
and report MPI process failures, it does not cope with daemon failures. 
%
 \orte~\cite{Castain05, Jeffrey12} is the \ompi runtime environment to launch, monitor, and kill parallel jobs, as well as managing I/O forwarding. It also connects daemons through various topologies, however the communication is not reliable. In general, these runtimes have limited applicability outside of the related \mpi implementation that has motivated their creation.

The \prrte runtime serves as the demonstrator and reference implementation
for the \pmix specification~\cite{CASTAIN18}. Technically it is a fork of
the \orte runtime, and thus inherit most of its capabilities to launch
and monitor \mpi jobs. Thanks to its well specified \pmix interface. \prrte
can deploy not only the related \mpi implementation, but also provides a
generic environment to deploy a wide variety of parallel applications (MPI based or not).
Although \prrte provides rudimentary support for clients fault detection and reporting, 
detection of failed nodes is rudimentary and slow, and the reporting 
broadcast topology is not itself resilient. 
The current work expands on the existing capabilities of \prrte by adding
advanced failure detection and reporting methodologies that can operate 
despite the failure of the runtime daemon themselves.

\subsection{Failure Detection}
Research in the areas of failure detection has been extensively studied. Chandra and Toueg~\cite{Chandra96} proposed the first unreliable failure detector oracle that could solve consensus and atomic broadcast problem for reliable distributed systems. Many implementations~\cite{Wei02, Larrea00, Kawazoe97} based on this oracle work are using all-to-all heartbeat patterns where every node periodically communicates with all nodes. However, those implementations are not scalable for lager systems with hundreds of nodes. The gossip-style protocol \cite{van98, Ranganathan01, Gupta01, Abhinandan02}, in which nodes pick at random other nodes to monitor and exchange information, is a popular approach for failure detection in unstructured systems were the group membership 
is not a-priori established. However, these gossip methods do not work well with very large number of node crashes, and the time to detect a specific failure is not strictly bounded given the 
random nature of the protocol. Furthermore, the gossip methods have the disadvantage of 
generating a large number of redundant detection and gossip messages that decrease the scalability.

Recently, our previous work \cite{George16} propose an observation ring based detector for HPC systems. 
the simulation results demonstrate the efficient of the algorithm, however the implementation in \ulfm is an application level failure detection strategy, which can only detect \mpi process failures. The 
implementation employs multiple optimization and shortcuts that are possible because it is 
tightly and deeply integrated within the \mpi library. For example, limitations on the 
accuracy of the detector when the \mpi implementation is not actively communicating 
are circumvented by using Remote Memory Access primitives (RMA) which are
initially provided for supporting the \mpi communication; the operational 
mode, overhead, and accuracy of the detector are impacted by the thread model used 
during the \mpi initialization (i.e., \mpifunc{THREAD_SINGLE} results 
in lower overhead but a higher chance of false positive than \mpifunc{THREAD_MULTIPLE}); 
and, in manycore systems, every \mpi process 
is observed and reported as an independent entity, which can impart that the 
overhead scales with the number of \mpi processes rather than the number of 
compute nodes. This resilient \prrte work 
exceed these limitations and has the capability to detect both process and 
node failures with a smaller observation topology, and is not limited to
MPI application only. 

\subsection{Reliable Broadcast}
Notable efforts have been made for fault-tolerant communication based on logical network typologies~\cite{Luo18}. A fully connected topology can handle large number of failures but it cannot be scalable since it sends too many messages. At the other extreme, a ring topology is good for scalability
but offers a poor propagation latency and suffers in scenarios with multiple node failures.
Circulant k-nomial graphs~\cite{Angskun07, Pava11} provide a balance between the previous two methods. 
Among circulant k-nomial graphs, the binomial graph (BMG) has the lowest diameter, which minimizes the number of hops for a dissemination to reach all 
processes. At the same time, binomial graphs scale well thanks to a regular, fixed degree graph (each node has the same number of neighbors). 
%
%The broadcast routing approach in~\cite{Angskun07} uses a collection 
%of binomial spanning trees with a fixed send ordering of neighbors. 
%The key-concept of fault-tolerant broadcasting is diameter of the topology, which is defined the longest shortest path between any two nodes in the graph. Binomial has the smallest diameter.
%Also binomial graph scales well with reasonable degree, regular graph and lowest diameter. 
%However when doing a reliable broadcast with message forwarding, 
%However when implementing a binomial graph, previous approach \cite{Angskun07} uses a fixed sending sequence to all the neighbours which has the disadvantage of not using high sending priority to unnotified nodes. Our work uses binomial graph with a reordering strategy when sending and forwarding messages, which means lower average reroute hops and message traffic density.

%AURELIEN: removed references to that aspect as the work is not mature enough and could be used in another future publication.
%Our work uses a BMG with a reordering strategy when sending and forwarding messages that 
%puts a high priority for messages that target leaves in previously effected 
%binomial trees, which helps reaching not yet notified nodes quicker when intermediate 
%hops have failed.\todo{that text needs another review/pass. But, not clear if want to talk about this in this %paper.}
 
\section{A Generic HPC Failure Detection Service}\label{sec:design}

In this section, we describe how we design a generic failure detector that can
be provided 
as an infrastructure service (that we call resilient-\prrte), while at the same time exploiting the 
specificities of the HPC machine model to sustain high detection accuracy 
and speed, while incurring a limited amount of noise on the monitored application. 

\subsection{Machine Model}

We consider a machine model representative of a typical HPC system. 
The machine is a distributed system comprised of compute nodes with an 
interconnection network. Each 
node can host runtime daemons and one or more application processes. Daemons 
and processes have a unique identifier (e.g., a rank) that can be used
to establish communication between any given pair. Messages take an unknown,
but bounded amount of time to be delivered (i.e., the network is pseudo-synchronous~\cite{Chandra96}).
 The identity and number of daemons and processes participating in the application is known a-priori, 
or is established through explicit operations that do not require group 
membership discovery. 

\subsection{Failure Model}

We strive to report crash failures, that is, when a compute entity stops emitting 
messages unexpectedly and permanently. A crash failure may manifest as
the ultimate effect of a variety of underlying conditions, for example, an illegal instruction 
is performed by a process because of a processor overheating, an entire 
node, or cabinet looses power, or a software bug manifests by interrupting 
unexpectedly, or rendering non-responsive some processes. In the context of 
this work, we further distinguish between two subtypes of crash failures.
First, application process failures, which may impact any number of 
hosted application processes without necessarily being concomitant
with the failure of other processes, even hosted on the same node.
Second, node failures, which we consider congruent with the observation of a daemon 
process failure. When a daemon failure occurs, all hosted application processes on
that node also undergo a process failure. We will discuss in the following 
sections how this distinction helps improve the scalability of the failure 
detection algorithm. 

\begin{table}
  \caption{Parameters and notations}\label{fig:notations}
  \label{tab:parameters}
  \begin{tabular}{ccl}
    \toprule
    Symbol & Description \\
    \midrule
    HNP & Head Node Process \\
    Daemon & Runtime environment process: the link  \\&  between the HNP and the application \\
    \texttt{\bf N} & Number of Daemon/nodes \\
    \texttt{Id} & The identifier of a Daemon \\
    \texttt{LIST\{ID\}} & List of known failed daemons' ID \\
    $\delta$ & Heartbeat period \\
    $\eta$ & Timeout for assuming a daemon failure\\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Notations}

Table~\ref{fig:notations} summarizes some of the notations we will employ 
to describe the algorithm. The HNP is the daemon node in charge of 
starting the initial deployment of the runtime environment and interfacing 
with the user (e.g., \emph{mpiexec}). The daemon is the infrastructure 
process deployed on each node to launch and monitor the execution of application 
processes on that node. The failure detector we propose employs heartbeats 
between daemons and timeouts to detect node failures.

\subsection{Detection of Process Failures}

As illustrated in Figure~\ref{fig:hosted}, the failure detector we propose 
employs two distinct strategies to detect process failures on one hand 
and node failures on the other hand. 

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{server_client.pdf}
  \caption{Hierarchical notification of hosted processes through \pmix notification routines. The \prrte daemon is in charge of observing, and forward notifications to the node-local managed application processes. The detection and reliable broadcast 
  topology operates at the node level between daemons.}\label{fig:hosted}
\end{figure}

To detect process failures that are not congruent with a node failure, we 
leverage on the direct observation of application processes that can be 
performed by the node-local daemon. Since a process failure does not 
impact the execution of the runtime daemon managing that process, that 
daemon can execute localized observation strategies which are dependent 
upon node-local operating system services. For example, the \orte Daemon 
Local Launch Subsystem (ODLS) monitors SIGCHLD signals to detect discrepancies 
in the core-binding affinity with respect to the user requested policy.
%Before the children processes execute the targets' executable, it need to set affinity of children processes according to a complex series of rules. This binding may fail in a myriad of different ways, the children processes will send a message to the parent indicating what happened (rendered error message) and the parent will read the message and analyze if this is a process failure and then use the runtime system's reporting mechanisms to display this error globally. 
That same signal also permits, from the node-local daemon, an extremely fast and efficient observation
of the unexpected termination of a local application process. As a substitute, 
or in complement, a daemon may also deploy a watchdog mechanism~\cite{CASTAIN18} 
to capture non-terminating crash failures that may arise from software 
defects, like live-locks, deadlocks and infinite loops. 

\subsection{Detection of Node/Daemon Failures}

Resilient \prrte's algorithm for node/daemon failure detection has two
components: a node-level observation ring, and a reliable broadcast overlay network between daemons.

We arrange all N daemons to a logistic ring topology, as illustrated in Figure~\ref{fig:Ring}.
Thus, each daemon \textbf{\textit{p}} observes its predecessor and is observed by 
its successor. The predecessor periodically sends heartbeat messages to \textbf{\textit{p}} (with a 
configurable period $\delta$). At the same time, \textbf{\textit{p}} sends heartbeat messages to its own observer. For each node, a daemon emits heartbeats $m_1$, $m_2$, ... at time $\tau_1$, $\tau_2$, ... to its observer \textbf{\textit{q}}. Let $\tau_i' = \tau_i + t$. At any time \textbf{\textit{t}} $\in [\tau_i', \tau_{i+1}')$, \textbf{\textit{q}} believes \textbf{\textit{p}} is alive if it has received the 
heartbeat message $m_i$ or higher. Otherwise, \textbf{\textit{q}} suspects \textbf{\textit{p}} has failed and \textbf{\textit{q}} initiates the dissemination of the error information of \textbf{\textit{p}} to all 
other daemons.

\begin{figure}[h]
\centering
\begin{minipage}[t]{.22\textwidth}
  \centering
  \includegraphics[trim=3cm 8.0cm 3cm 8cm,width=\linewidth]{ring_detector.pdf}
  \captionof{figure}{Daemons monitor one another along a ring topology to detect node failures.}
  \label{fig:Ring}
\end{minipage}%
\hfill
\begin{minipage}[t]{.22\textwidth}
  \centering
  \includegraphics[trim=3cm 8.0cm 3cm 8cm,width=\linewidth]{reconnet_cross.pdf}
  \captionof{figure}{The algorithm mends the detection ring topology when a node failure occurs by requesting heartbeats from the closest live ancestor in the ring.}
  \label{fig:ReconnectRing}
\end{minipage}
\end{figure}


When the observer detects that its predecessor has failed, it undergoes two major steps.
First it needs to reconnect the ring topology, as illustrated in Figure~\ref{fig:ReconnectRing}. Daemon \textbf{\textit{q}} considers
 its own list of known failed daemon and elects to observe the first 
 daemon \textbf{\textit{nq}} following the predecessor relationship on the ring that does not appear in the failed list. It sets \textbf{\textit{nq}} as its predecessor and then send a request to \textbf{\textit{nq}} for initiating the heartbeat emission. Z global mapping and binding information and find out all those processes hosted on predecessor node.\todo{what?} After we get the list of all those affected processes and nodes, the observer calls the propagation component to broadcast the fault information to other daemons, and then notify its local processes. Then, the observer updates its local known failure List\{\textit{ID}\}.
 
  %For all those nodes who received the notification, each node need to forward this information, maintain its own List\{\textit{ID}\} and notify locally. As show in figure \ref{fig:prrte}, we create a new module in error management as detector to enable this feature.

\subsection{Broadcasting Fault Information}
Considering that the observation topology is static, it does not provide 
automatic or probabilistic dissemination of fault information. Thus, to complete 
the reporting of failures, failures identified by an observer must be broadcasted
to inform all other daemons and application processes. An important aspect,
when considering a runtime that tolerate node/daemon failures, it that the 
propagation algorithm itself needs to be resilient to failures. 

For broadcasting fault information between daemons, we use the scalable and fault 
tolerant BMG topology~\cite{Angskun07}. BMG has good fault-tolerant properties such 
as optimal connectivity, low fault-diameter, strongly resilient and good optimal 
probability in failure cases. Note that unlike prior works, the propagation 
algorithm~\ref{fig:2lvlbmg} is not a flat BMG when application processes are considered, but consists of an 
inner BMG overlay between daemons, and an outer star overlay from each daemon to its 
local managed processes.

\begin{algorithm}
\caption{Two-Level Reliable Broadcast Algorithm}\label{fig:2lvlbmg}
\textbf{\textit{N}} \Comment{Number of alive nodes}\newline
\textbf{List\{$EID$\}} \Comment{List of locally known
failed processes' identifiers}\newline
\textbf{\textit{msg}} \Comment{Message containing failure affected process identifiers and state}\newline
\textbf{\textit{flag}} \Comment{Boolean flag indicating whether to forward the message or not}\newline
\textbf{List\{$AID$\}} \Comment{Processes identifiers' hosted on failed daemon identifier }\newline

\begin{algorithmic}[1]
\Procedure{Broadcast}{ $i, N, msg$ }\Comment{Daemon \textbf{i} send error message to all its neighbours}
\For{ $k \gets 0$ to $\log_2 N$ }\Comment {Sending order not fixed}            
     \State {\textbf{i} send \textbf{msg} to  ( ($N$ + i + $2^k$) \textbf{mod} {N} ) }
     \State {\textbf{i} send \textbf{msg} to  ( ($N$ + i - $2^k$) \textbf{mod} {N} ) }
\EndFor
\EndProcedure
\end{algorithmic}

\begin{algorithmic}[1]
\Procedure{Forwarding}{ $flag, msg$ }\Comment{Daemon \textbf{j} receives msg, forwarding and notify locally }
\If {$flag == true$}
    \State Broadcast( $j, N, msg$ )
    \State Update \textbf{List\{$EID$\}}
    \State Notify locally
\EndIf
\EndProcedure
\end{algorithmic}

\begin{algorithmic}[1]
\Procedure{Start propagation}{ $job, Eid, state$ }\Comment{Daemon \textbf{j} start propagation }
\If {( $Eid$ $\notin$ List\{$EID$\} )}
    \State Add $Eid$ to $msg$
    \State Get \textit{AID} and adde to $msg$
    \State Broadcast( $j, N, msg$ )
    \State Add $Eid$ \textit{AID} to List\{$EID$\}
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

Figure \ref{fig:bmg} shows an example of the execution of the BMG broadcast 
with 12 nodes. For simplicity, the local start stemming from each daemon are not 
represented. 

\begin{enumerate}
 \item In this example, daemon 0 is the initial reporter and its observer component starts the propagation by calling the \textsc{Start Propagation} reliable broadcast algorithm.
 \item This prepares a broadcast message containing the identifier of the 
 failed process (or daemon). daemon 0 issues the direct propagation to the local application processes it manages (if any still live).
 \item Upon receiving a broadcast message, a daemon considers if the message needs 
 to be forwarded. If the message carries a list of processes that are already known to
 have failed, then, the daemon already triggered the propagation, and no further 
 action is needed. 
 All nodes forwarding the message upon receiving to make the broadcast reliable. Make sure all alive nodes receive the information at least once. 
\end{enumerate}






\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{BMG_seq.pdf}
  \caption{Binomial graph with 12 nodes}
  \label{fig:bmg}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{reorder_span.pdf}
  \caption{Broadcast using binomial spanning tree from node 0, extra messages to neighbours are colored in blue }
  \label{fig:reorder_span}
\end{figure}




The broadcast algorithm sends a message to all its six neighbors in the order of the marked sequences. This initiates the following sequence 




And Figure \ref{fig:reorder_span} shows the a spanning tree for broadcast originated from node 0 also with extra messages colored blue for resilient purpose. Those extra messages balanced the trade-offs between high-radix trees that allow more parallelism at each level in the tree with good network, and lower-radix deeper communication trees with reduced network congestion. The advantages of this new broadcast algorithm are:
\begin{enumerate}
  \item Excellent network connection brings higher parallelism: message to node \{10, 11, 7\} could arrive from any forwarding nodes at earlier time stamp than routed from spanning tree, this decrease the height of the tree, the average latency and the maximum notification latency are smaller.
  \item With limited network connection: maintains the upper bound of broadcast to $O(log_2^{N})$. 
\end{enumerate}



 


Group communication use out-of-band non-blocking peer to peer communicate channel as the basic message exchange mechanism, we assume the communication latency from any two directly connected nodes with same message size are equivalent. For node level broadcast, each node sends message to all its neighbors  as in BMG topology, message from any node we follow the sequence as in binomial spanning tree by which means that any node in BMG can always be delivered within $O(log_2^N)$ steps.


\begin{figure*}[h]
\centering
\begin{minipage}{.37\textwidth}
  \centering
  \includegraphics[width=\linewidth]{multi_pingping_pingpong_overhead.pdf}
\end{minipage}%
\begin{minipage}{.63\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Bcast_overhead_with_ulfm_max_col.pdf}
\end{minipage}
\captionof{figure}{\prrte with fault tolerance overhead over \prrte and \ulfm using IMB}
\label{fig:overhead}
\end{figure*}

For inter-node notification, children processes subscribe to a event handler with specific error code and will be triggered by the notification function when a daemon doing the locally notification. Also,applications can register to multiple events which enable the capability to receive notification from different jobs. This reliable broadcast feature is implemented in group communication and propagation component.



\begin{figure}[h]
  \centering
  \includegraphics[trim=0.2cm 9.0cm 0.2cm 9cm,width=\linewidth]{PMIx_PRRTE.pdf}
  \caption{Resilient \prrte component architecture. The orange boxes represent the component we mainly use to add the resilient features. The dark blue colored boxes are new modules}\label{fig:prrte}
\end{figure}

\subsection{Implementation}

 \prrte is a fork of the \ompi runtime, \orte~\cite{Castain05}. It is based on a Module Component Architecture (MCA) which permits easily extend or substitute the core subsystem with experimental features. As shown in in figure~\ref{fig:prrte}, within this architecture, each of the major subsystems is defined as an MCA framework, with a well-defined interface. Also each framework contains one or more components each representing a different implementation. Then the \orte work spun off to its own effort as a ``\verb|shim|`` between the application and the host environment that includes full support for the \pmix \cite{CASTAIN18} - an abstract set of interfaces by which not only applications and tools can interact with the resident system management stack (SMS), but also the various SMS components can interact with each other. 

\prrte provides an easy way of launching, running and monitoring \pmix-based applications outside of a \pmix-enabled environment on distributed systems. The \prrte overall architecture is out the scope of this paper, but we still want to mention some features embraced by it. First of all, \prrte's very first feature is Job controlling and Monitoring~\cite{Ralph15} which enables the application and SMS to coordinate the response to failure: termination of the job or a subset of processes, request replacement of nodes and processes, or continue execution at a lower setting. This provides the foundation of resilience strategy of our work. Another important feature is the \pmix Event Notification~\cite{Ralph002} : the resource manager or server can notify the application of events, the application processes can notify their server or SMS of issues. This provides the channel for locally propagation of error messages. Resilient \prrte mainly embraces two new features of failure detection and reliable broadcast for messages propagation globally and locally. We will give a detail introduction of the two features in the following sections.

\section{Experiment Evaluation}\label{sec:experiments}

\subsection{Experiment Setup}
Experiments are conducted on two different machines: (1) \todo{more detail about nacl}a local cluster named NaCl, an Intel Xeon X5660 machine with 66 nodes, 12 cores per node. (2) NERSC's Cori
%~\cite{Cori01} 
-- a Cray XC40 with Intel Xeon "Haswell" processors and the Cray "Aries" high speed inter-node network, 32 cores per node. Our \ourwork is based on \prrte(\#71ef547), with external \pmix(\#21d7c9). The comparison \ulfm is based on(\#77f9157). The experiments is repeated 30 times and we present the average. We use Intel MPI Benchmark (\imb v2019.2) \cite{IMB} for MPI performance measurements for point-to-point (P2P) and collective communications and deploy with one \mpi rank per core. For all experiments we use map-by node, bind-to core binding policy -- put sequential MPI processes on adjacent processor cores. The only exception is the \imb P2P experiment we use map-by node, bind-by node -- put sequential MPI processes on adjacent nodes. \todo{add binding policy}

\subsection{Accuracy and Noise}
For the first experiment we want to implore the accuracy of \ourwork's detector with different heartbeat periods. We also investigate the noise overhead generated by the heartbeat ring with different heartbeat periods from milliseconds to seconds.

The accuracy experiment is conducted as: (1)\ Starting with considerable value of $ \eta $, then decrease $ \eta $ and corresponding $ \delta $ until we notice false positive. The default setting of $ \eta $ and $ \delta $ is $ \eta = \delta * 2 $. (2) Apply a reasonable fixed value of  $ \delta $ , decrease  $ \eta $ until node misses the deadline of a heartbeat sending.  \todo{pull the description of the methodology in to make it self contained} If the test is successful (no failure is detected when there is no injection, and all injected failures are correctly detected), then we decrease the heartbeat period and repeat the test until a false positive is reported. We explore \ourwork's detector accuracy with two experiments using 64 nodes on NaCl: 
\begin{enumerate}
  \item No daemon failure is injected but detector detects daemon failure, which means a observer doesn't receive any heartbeat and timeout is reached. We explore the possible reasons with two tests: (1) Test with no communication application, all tests succeed until heartbeat period under 20 milliseconds. (2) Test with \imb with heavy P2P communication and collectives, all tests succeed until the heartbeat period lower than 20 milliseconds as above. We assume that the timeout is neither delayed by communication congestion nor compute pressure. We investigate the problem with smaller job size running on less nodes, it shows a better performance. We find out that daemons need some time to launch the processes when starting the job which caused the delay. 
  \item No failure is injected and node missed its heartbeat sending deadline. With a reasonable timeout value, all daemons can send heartbeats successfully with $ \delta $ as low as 0.1 millisecond. 
\end{enumerate}

From all above we can see that the granularity of timeout is 40 milliseconds limited by the latency for daemons to launch the application processes. And the granularity of heartbeat period is 0.1 millisecond.  

Figure \ref{fig:overhead} illustrates the overhead incurred with P2P and collective communications running \imb. 
%All IMB-MPI1 suits can be successfully conducted with no false detection with $ \delta \geq 1 ms $.
For the P2P setting, we run the benchmark on two nodes with all cores included using the \imb P2P's "multi" option, which pairs cores from each node as groups to ensure that no matter how daemons are binded to cores the heartbeat emissions affect the performance of P2P communication. For the collectives we run the benchmark on 64 nodes using all cores. Any single test we use message size from several bytes to mega bytes, for each message size the test lasts more than 20 seconds to ensure enough heartbeat emissions are occurred during the experiment. The \prrte overhead is calculated by using the latency (LAT) results of benchmark suits: 
\begin{equation}
\frac{( {\ourwork-LAT - PRRTE-LAT} )}{PRRTE-LAT}
\end{equation}
 From the figure we can see that the latency performance and bandwidth performance are barely affected by heartbeats periods from milliseconds to seconds. Notably, when $ \delta \geq 10 ms $, it has trivial influence on the system. When  $ \delta = 1 ms $ the noise overhead for P2P is less than three percentage. As for collective communication (Bcast, Reduce and Allreduce) the noise overhead are less than eight percentage under the circumstance that the standard deviation of the benchmark itself is four percentage. Also we calculate the overhead of \ulfm without fault tolerance compared to \prrte, shows that \ulfm has about two percent overhead for P2P and collective communication respectively for both small and big message sizes.

\subsection{Comparison with SWIM}
This section compares failure detection latency and scalability of \ourwork with SWIM~\cite{Abhinandan02} -- using a random-probing based failure detection protocol and disseminates membership updates via network multi-cast. SWIM uses subgroups to probe to decrease the randomness and increases the scalability. To avoid false detection, SWIM uses suspicion mechanism, when a node does not reply direct or indirect probing in time, the initiator then judges this node as suspicious instead of failure, then broadcasts
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{HB_prrte_swim.pdf}
  \caption{Detection and Propagation delay compared to SWIM using Go-Memberlist}
  \label{fig:hb_prrte_swim}
\end{figure}
this suspicious information within the subgroup, if any node in the subgroup receives an acknowledge before the timeout, it will unmask the suspected node as alive, otherwise mark as failure. In order to improve the efficiency of multi-cast, SWIM uses a infection-style dissemination mechanism as information spreads in a manner analogous to the spread of gossip in society, or epidemic in the general population.

Figure \ref{fig:hb_prrte_swim} compares detection and propagation latency between \ourwork and SWIM of node failure. For the SWIM implementation, we use Go-Memberlist (\#a8f83c6) which is integrated with Go instead of mpi, however we use go-mpi binding to enable SWIM run as mpi application, we use mpi barrier to synchronize before injecting failures in SWIM, and SWIM reports failure through Go-Memberlist callback functions. We run all Memberlist tests only up to 256 processes which is the upper bound of its members limited by maximum connection requests on Transmission Control Protocol (TCP) socket, also this implementation suffers from connection storm and cause consequently crash of continuous running. 

For \prrte, we also synchronize with barrier, and then inject node failure, detection and notification latency are collected on each process. We run all tests up to 768 processes on 64 nodes. We compare the latency of detection and propagation with different heartbeat period values, for all tests we set $ \eta = \delta * 2 $. We can clearly see that for \prrte the detection latency is between ($\delta$, $2*\delta$), and the receive of notification happens immediately after the detection which demonstrate the efficiency of our propagation algorithm. However for SWIM even with smaller number of processes the delay is still more than $10*\delta$. The variation of \prrte result comes from the randomness of when the node failure happened.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Scale_prrte_swim.pdf}
  \caption{Scalability compared to SWIM using Go-Memberlist}
  \label{fig:scale_swim}
\end{figure}

Figure \ref{fig:scale_swim} compares the scalability of the two detectors regard to the number of deployed processes. We set the heartbeat period to 0.5s with timeout of 1s. For smaller number of processes \ourwork is stabilizing in approximately 0.75s, a statistic behavior of the randomness of when the failure happened between two contiguous heartbeats, while the stabilization delay of SWIM is more than 10 times of heartbeat period. As the number of processes increases, \prrte's latency remains almost the same. But SWIM shows a linear increase which will be the bottleneck of scaling up (with the assumption it can solve the maximum connection requests limit). For \prrte with 4K processes the stabilization is still beyond the range of heart period and timeout.

\subsection{Comparison with \ulfm for Process Failures}
This section compares \ourwork with our previous work \ulfm, the \ulfm implementation also has two main components: process level detection ring, and propagation overlay with all launched processes. The detection ring is built at Byte Transfer Layer (BTL) level - which provides the portable low-level transport abstraction in \ompi. The current implementation provides several mechanisms to ensure the timely activation and delivery of heartbeats:
\begin{enumerate}
  \item Using a separate, library internal thread to send the heartbeats in order to be separated from the application's communication, this also avoid the possible heartbeat delay of compute intensive application. For receiver it need to poll BTL engine to check the aliveness of its successor. 
  \item Using RDMA put to raise a flag in the receiver's registered memory, by using the hardware accelerated put operations solves the problem of active polling BTL engine. 
  \item Using in-band detection for process to report unreachable error directly to the propagation component.
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Process_Failure_log_fit.pdf}
  \caption{Process failure detection and propagation delay compared to \ulfm}
  \label{fig:proc_failure_nacl}
\end{figure}

The propagation overlay is also built at BTL level, the small message size of propagation information contains a callback function index which ensures that the received process states update information can be analyzed by upon reception. This method provides independence from the \mpi semantic (including matching), however the overlay is constructed with all processes, which means that all participated processes need to propagate/forward the error information, the lower bound of reaching any process is bounded by $\log_2({Number\ of\ Processes})$.  

However, \prrte process failure detection is implemented as the daemons are monitoring the lifeline of all local processes, this mechanism doesn't bring any pressure to the application' communication resources, also doesn't need RDMA hardware accelerate support. About the propagation strategy of \prrte, the broadcast overlay is built at daemon level which highly decreases the number of participates, with less participates the total messages transferred and forwarded are much less than the case of \ulfm, the lower bound is $\log_2({Number\ of\ Nodes})$. With the development of more powerful multi-core nodes, the benefit of node level propagation will be much more significant.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Cori_Process_Failure_fit.pdf}
  \caption{Process failure detection and propagation delay on Cori}
  \label{fig:proc_failure_cori}
\end{figure}

Figure \ref{fig:proc_failure_nacl} compares the latency of process failure detection and propagation of \ulfm and \prrte. The sensitivity of \ulfm heartbeat without false detection is 10 milliseconds, but we want to compare the best performance from \ulfm using high speed in-band detection which is faster than using the lowest heartbeat period. For \prrte, the daemons are in charge of detecting process failure. \prrte uses TCP to broadcast among daemons, daemons use \pmix's notification method to distribute the error information to all hosted processes. Experiments are conducted on NaCl from 2 nodes to 64 nodes using all cores on each node, all processes are equalled mapped to node and bind to core, by doing this \ulfm can use the in-band shared communication for detection and propagation. We can see that our implementation gains almost the same performance as \ulfm but highly reduced the complexity, the detection and propagation time is less than 5 milliseconds using TCP. For \ulfm the detection and propagation delay increase from 2 milliseconds to 3 milliseconds as the processes number increases with number of nodes using infiniband. For both \prrte and \ulfm the latency increase trend fit to $ a*\log_2(N) + b $, which can be easily scale up to hundreds of thousands of nodes. Similar result in figure \ref{fig:proc_failure_cori} from Cori (with more processes on each node) strengthens the efficiency our implementation, we can see that with 4K processes the detection and propagation latency is about 10 milliseconds. Same results from two different machines demonstrate the efficiency of our broadcast and propagation algorithm.  

\subsection{Node Failures Detection}
Future more, we want to extend the performance of our heartbeat detector  for node failure detection and propagation from  three perspectives. 
\begin{enumerate}
  \item Latency of detection time and propagation time with different heartbeat periods.
  \item The detection and propagation efficiency using fixed heartbeat and timeout period but with different number of nodes.
  \item Detection latency of multiple node failures.
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{PRRTE_ULFM_comparision.pdf}
  \caption{Single Daemon Failure detection and propagation delay compared to \ulfm with different heartbeat period}
  \label{fig:node_failure_hb}
\end{figure}

Figure \ref{fig:node_failure_hb} presents the behavior observed when injecting single daemon failure under different heartbeat period setting. We conducted the experiments on 64 nodes with 764 processes. For \ourwork after synchronizing, we inject a node crash by choosing a process to kill its host using the parent process identifier. However, \ulfm doesn't have the capability of detecting node failure, we simulate "node crash" by kill the last process on that node. With the process ring detector this particular process failure will be detected by its observer on another node, this behavior is the same as \ourwork's node to detect node failure. Also we assume that the time different between the daemon failure and process failure is trivial which is confirmed later in the experiment. For the heartbeat period setting we start at 30 milliseconds to 0.5 second for both \ourwork and \ulfm. For all heartbeat periods we set $ \eta = \delta * 2 $. From the figure we can see that the latency for all cases are bound by $ (\delta,\eta) $.

Figure \ref{fig:daemon_failure_diffnode} shows single node failure detection and propagation performance with fixed heartbeat period $ \delta = 0.5s $ tested with different number of nodes. After a node crash, all processes hosted on this node will be affected, the observer node fetches and packs the information of all affected processes information then distribute the packed message.
\textbf{
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Daemon_prrte_only.pdf}
  \caption{Single Daemon Failure Detection and Propagation delay with different number of nodes}
  \label{fig:daemon_failure_diffnode}
\end{figure}}
However, for \ulfm all affected processes need be detected by their observers independently, this will suffer from the collisions on the reliable broadcast propagation. For the worst case detection, all affected processes are adjacent in the observation ring, for each affected process the ring need to re-connect and update, causes a linear increase of $ \eta $ to the detection latency for every process failure. From the figure see that \ourwork can detect and propagate a node failure between (0.5s, 1s) for all tested number of node. 

The last experiment (Figure \ref{fig:multi_daemon_failure_nacl}) investigates the effect of multiple node failures happened in the system. The test setting is the same as single node failure case, except we inject multiple node failures from processes. For the first scenario, injecting failure to nonadjacent nodes, the detection and propagation are independently conducted by different observer nodes which is a parallel execution. And the results in the figure show that the latency is a constant value which is barely affected by the number node failures. The worst scenario is to inject failures to contiguous nodes, all failures are detected by a single observer (switches to observing the predecessor of the detected failed node). This shows a serial execution. For each node failure will cause an extra delay of $ \eta $ to the detection latency of the following node failures. From the figure we can see the result matches a linear increase. This extrapolate that from fault tolerance perspective the system will be more stable and reliable by building the detection ring in a arbitrary layout of nodes, avoid choosing predecessor and successor from the same cabinet.  

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{multi_daemon_failures.pdf}
  \caption{Multiple daemon failures at the same time}
  \label{fig:multi_daemon_failure_nacl}
\end{figure}

\section{Application}\label{sec:motivation}
In this section, we present the use case of the \ourwork.
\textbf{\ulfm:} \ourwork supports the functionality in \ulfm of revocation and correction of a broken communicator after node failure. For \ulfm the global communicator could be reconfigured after process failure detection, the failed processes are excluded from the global communicator using the \mpifunc{Comm_shrink} and are re-spawned using the \mpi function \mpifunc{Comm_spawn}, but the job will be aborted with node failure. However, using \ourwork instead of \ompi runtime the job doesn't need to be terminated after node failure,the runtime level propagation mechanism deliver error message to the job scope. With the notification \mpi application can get the consensus knowledge of failure, then application can use the recovery functions (Shrink, Respawn) provided by \ulfm to build new communicator without the processes on the failed node but with those newly spawned processes. 
\todo{add SHMEM, REINIT; thinking about moving this part as 2. Motivation}

\textbf{\oshmem:} fault tolerance model is based on check-point and restart, that suits to the one-sided nature of PGAS programming model while leveraging features very specific to \oshmem. \ourwork failure detection and propagation attributes provide the flexibility to application developers to modulate the frequency and placement within the application where the checkpoint may be introduced. Also the notification provides the trigger for recovery. 

\textbf{EREINIT:} a global-restart failure recovery model by allowing a fast re-initialization of \mpi. This work is a co-design between MVAPICH and Slurm resource manager to add process and node failure detection and propagation features. The detection is using Slurm's health check mechanism by sending ping messages to nodes, propagation is implemented in a manner of forcing the controller to individually send the notification to children of the failed nodes. However this work is highly depends on one particular resource manager Slurm has the limitation to run only on Slurm machine, also it use a inefficient propagation method. \ourwork work can exceed the bottleneck of EREINIT to run on machines with different resource managers(Slurm, PBS, LSF, TORQUE, etc). Also with our efficient propagation algorithm the notification will be much faster advances the stabilization and recovery time of EREINIT.

\section{Conclusion}\label{sec:conclusion}
Failure detection and propagation is a critical service for resilient systems. In this work, we present an efficient failure detection and propagation design and implementation for distributed systems integrated within \pmix\_\prrte. The process and node failure detection strategy presented in this work depends on heartbeats and timeouts, and communication bound to provide a reliable solution that works at scale without constraints of the kind of faults. Our design and implementation takes into account of the complicate relationship and trade-off between system overhead, detection efficient and risks: low detection time requires frequent emission of heartbeats messages, increasing the system noise and the risk of false positive. Our solution addresses those concerns and capable of tolerating high frequency node and process failures with the help of high performance reliable broadcast notification can quickly disseminate the fault information, our results from different machines and benchmarks compared to two other related work shows that \ourwork significantly advances the state of art with respect to efficient detection and propagation of failures. This runtime level failure tolerate implementation breaks the limit of running \mpi  application only and support all \pmix-based application which is more extensive than \mpi scope.   


%\section{Acknowledgments}
%
% The acknowledgments section is defined using the "acks" environment (and NOT an unnumbered section). This ensures
% the proper identification of the section in the article metadata, and the consistent spelling of the heading.
\begin{acks}
%CAARES and ECP OMPIX
This material is based upon work supported by the National Science Foundation under Grant No. (1725692); and the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the 
U.S. Department of Energy Office of Science and the National Nuclear Security Administration.    
\end{acks}

%
% The next two lines define the bibliography style to be used, and the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}
% 
% If your work has an appendix, this is the place to put it.
\appendix
\end{document}
